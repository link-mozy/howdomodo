{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql.types import *\n",
    "from functools import reduce\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import Row\n",
    "import pyspark.sql.functions as f\n",
    "import time\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext('local')\n",
    "sqlContext = SQLContext(sc)\n",
    "spark = SparkSession(sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Path does not exist: file:/C:/ssafy/2nd/s03p23a305/bigdata/analysis/2019-09-UTF.csv;",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-47f234a2a67a>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mlistname\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mcsv_list\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msqlContext\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'com.databricks.spark.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minferSchema\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'true'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'C:/ssafy/2nd/s03p23a305/bigdata/analysis/2019-'\u001b[0m\u001b[1;33m+\u001b[0m\u001b[0mlistname\u001b[0m\u001b[1;33m+\u001b[0m\u001b[1;34m'-UTF.csv'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[0;31m\\\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\readwriter.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self, path, format, schema, **options)\u001b[0m\n\u001b[0;32m    176\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0moptions\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    177\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbasestring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 178\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_df\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jreader\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    179\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mpath\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    180\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\py4j\\java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1302\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1303\u001b[0m         \u001b[0manswer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1304\u001b[1;33m         return_value = get_return_value(\n\u001b[0m\u001b[0;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n\u001b[0;32m   1306\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mdeco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    132\u001b[0m                 \u001b[1;31m# Hide where the exception came from that shows a non-Pythonic\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m                 \u001b[1;31m# JVM exception message.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 134\u001b[1;33m                 \u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mconverted\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    135\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m                 \u001b[1;32mraise\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pyspark\\sql\\utils.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(e)\u001b[0m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Path does not exist: file:/C:/ssafy/2nd/s03p23a305/bigdata/analysis/2019-09-UTF.csv;"
     ]
    }
   ],
   "source": [
    "csv_list = [\"09\",\"10\",\"11\",\"12\"]\n",
    "df_list = []\n",
    "activityList = [1001,1003,1004,1007,1008,1010,1099,1101,1102,1104,1105,1199,1201,1202,1203,1204,1205,1206,1207,1208,1299,2002,2003,2004,2099,2104,2107,2109,2110,2111,2112,2113,2199,2401,2402,2403,2404,2406,2407,2499,2299,2312,2317,4113,7104,7299,6001,6002,6003,6004,6005,6006,6007,6008,6009,6010,6011,6012,6013,6015,6016,6021,6099,6014,6019,6020,6018,6107,6113,6201,6202,6203,6204,6205,6206,6207,6208,6209,6210,6111,6112,6299,7101,7102,7103,7105,7106,7108,7199]\n",
    "\n",
    "for listname in csv_list:\n",
    "    df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true')\\\n",
    "        .load('C:/ssafy/2nd/s03p23a305/bigdata/analysis/2019-'+listname+'-UTF.csv')\\\n",
    "        .cache()\n",
    "    \n",
    "    # 기업 데이터 제거\n",
    "    df = df.filter(df.개인기업구분 != \"기업\")\n",
    "    df = df.filter(df.성별 != \"0.기타\")\n",
    "    \n",
    "    # 활동 분류\n",
    "    df = df.filter(df.가맹점업종코드.isin(activityList))\n",
    "    \n",
    "    removeColList = [\"회원_시도명\", \"회원_시군구명\", \"개인기업구분\", \"매출년월일\", \"결제금액\", \"결제건수\"]\n",
    "    for ls in removeColList:\n",
    "        df = df.drop(ls)\n",
    "    df.persist()\n",
    "    df_list.append(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_list[0].show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema = StructType([StructField(\"성별\", StringType(), True), StructField(\"연령\", StringType(), False),\\\n",
    "#                      StructField(\"승인시간대1\", IntegerType(), True), StructField(\"가맹점업종코드\", IntegerType(), True)])\n",
    "\n",
    "# df_all = sqlContext.createDataFrame([],schema)\n",
    "df_all = df_list[0]\n",
    "# df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx, df in enumerate(df_list):\n",
    "    if(idx == 0):\n",
    "        continue\n",
    "    print(idx)\n",
    "    df_all = df_all.union(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_all.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all.colRegex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_all_renamed = df_all.withColumnRenamed(\"성별\", \"gender\")\\\n",
    "                        .withColumnRenamed(\"연령\", \"age\")\\\n",
    "                        .withColumnRenamed(\"승인시간대1\", \"time\")\\\n",
    "                        .withColumnRenamed(\"가맹점_시도명\", \"do\")\\\n",
    "                        .withColumnRenamed(\"가맹점_시군구명\", \"si\")\\\n",
    "                        .withColumnRenamed(\"가맹점_읍면동명\", \"dong\")\\\n",
    "                        .withColumnRenamed(\"가맹점업종코드\", \"code\")\\\n",
    "                        .withColumnRenamed(\"회원수\", \"n\")                        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_all_renamed.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = df_all_renamed.rdd\n",
    "rdd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd = rdd.map(lambda x: Row(gender=int(x[0][0]),\\\n",
    "                            age=int(x[1][0]),\\\n",
    "                            time=int(x[2]),\\\n",
    "                            do=x[3],\\\n",
    "                            si=x[4],\\\n",
    "                            dong = x[5],\\\n",
    "                            code =int(x[6]),\\\n",
    "                            n = int(x[7])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rdd.take(3)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "type(rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([StructField(\"gender\", StringType(), True),\\\n",
    "                     StructField(\"age\", StringType(), True),\\\n",
    "                     StructField(\"time\", IntegerType(), True),\\\n",
    "                     StructField(\"do\", StringType(), True),\\\n",
    "                     StructField(\"si\", StringType(), True),\\\n",
    "                     StructField(\"dong\", StringType(), True),\\\n",
    "                     StructField(\"code\", IntegerType(), True),\\\n",
    "                     StructField(\"n\", IntegerType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_mapped = spark.createDataFrame(rdd, schema).cache()\n",
    "df_mapped = spark.createDataFrame(rdd, schema).persist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캐싱 없이 쇼\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_mapped.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캐싱 없이 재차 쇼\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_mapped.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 캐싱 후 쇼\n",
    "df_mapped.persist()\n",
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_mapped.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_mapped.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_mapped.unpersist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_mapped.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_mapped.registerTempTable(\"df_tmp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "df_mapped.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 코드 형식\n",
    "# df_mapped.groupBy('gender', 'age','time', 'code').count().select('gender','age','time', f.col('count').alias('n')).show()\n",
    "\n",
    "# df_mapped.groupBy('gender', 'age','time', 'code').sum('n').show() # count().select('gender','age','time', f.col('count').alias('n')).show()\n",
    "df_group = df_mapped.groupBy('gender', 'age','time', 'do', 'si','dong', 'code').sum('n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 쿼리 형식\n",
    "df_group = sqlContext.sql(\"select *, sum(n) as cnt from df_tmp group by gender, age, time, code\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group = df_group.withColumnRenamed(\"sum(n)\", \"total\").persist()\n",
    "df_group.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "df_group.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1163345"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_group.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_group.registerTempTable(\"df_group\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dong = \"'영등포동'\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"select gender, age, time, code, total from df_group where dong ='영등포동' order by total desc\""
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"select gender, age, time, code, total from df_group where dong =\"+ dong +\" order by total desc\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df = sqlContext.sql(query).persist()\n",
    "\n",
    "# selected_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_df.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gender', 'age', 'time', 'code', 'total']"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "selected_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "selected_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_df.registerTempTable(\"selected_df\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'select code, sum(total) as sum from selected_df group by code order by sum desc'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "query = \"select code, sum(total) as sum from selected_df group by code order by sum desc\"\n",
    "query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sum_groupByDf = sqlContext.sql(query).persist()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+\n",
      "|code| sum|\n",
      "+----+----+\n",
      "|2099|7240|\n",
      "|2004|5873|\n",
      "|2003|5731|\n",
      "|2199|4043|\n",
      "|6014|3107|\n",
      "|2104|3013|\n",
      "|2002|1380|\n",
      "|7106|1047|\n",
      "|1004| 989|\n",
      "|2406| 211|\n",
      "|4113|  69|\n",
      "|2402|  65|\n",
      "|2112|  52|\n",
      "|7103|  21|\n",
      "|7102|  15|\n",
      "|2109|   3|\n",
      "+----+----+\n",
      "\n",
      "time : 17.43963932991028\n"
     ]
    }
   ],
   "source": [
    "start = time.time()  # 시작 시간 저장\n",
    "\n",
    "sum_groupByDf.show()\n",
    "\n",
    "print(\"time :\", time.time() - start)  # 현재시각 - 시작시간 = 실행 시간"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1964088"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_sum = 0\n",
    "for row in selected_df.collect():\n",
    "    total_sum += row.total\n",
    "total_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "michelin = spark.read.json('C:/ssafy/2nd/s03p22a305/bigdata/analysis/data_data_josn')\n",
    "michelin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "code_df = sqlContext.read.format('com.databricks.spark.csv')\\\n",
    "        .options(header='true', inferSchema='true')\\\n",
    "        .load('C:/ssafy/2nd/s03p22a305/bigdata/analysis/codelist-UTF.csv')\\\n",
    "        .cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+-----------+------------------+----+----------------------------------+\n",
      "|업종분류|       업종|                업|코드|                  적용범위 및 기준|\n",
      "+--------+-----------+------------------+----+----------------------------------+\n",
      "|  의생활|       의류|          맞춤복점|1001|남성복 맞춤업소,여성복 맞춤업소...|\n",
      "|  의생활|       의류|            한복점|1003|  남.여 한복 맞춤업소, 개량한복...|\n",
      "|  의생활|       의류|          기성복점|1004|  의류대리점(모피 및 교복 포함)...|\n",
      "|  의생활|       의류|  아동 및 유아복점|1007|           아동복 및 유아복 판매점|\n",
      "|  의생활|       의류|        내의판매점|1008|    내의,타월,양말,스타킹등 판매점|\n",
      "|  의생활|       의류|            양품점|1010|             각종 양품,잡화 판매점|\n",
      "|  의생활|       의류|         기타 의류|1099|                              null|\n",
      "|  의생활|직물/침구류|        옷감판매점|1101|  양복,양장,한복등의 옷감 및 면...|\n",
      "|  의생활|직물/침구류|침구,커튼,카펫트점|1102|  이불,담요,베개,카바,커튼,카펫...|\n",
      "|  의생활|직물/침구류|            수예점|1104|   각종 수예품(자수,실포함) 판매점|\n",
      "|  의생활|직물/침구류|            자석요|1105|              자석요 수입,제조판매|\n",
      "|  의생활|직물/침구류|         기타 직물|1199|                              null|\n",
      "|  의생활|   신변잡화|        악세사리점|1201|패션주얼리전문점(예:미니골드 등...|\n",
      "|  의생활|   신변잡화|          귀금속점|1202|   금,은,보석등 (귀금속 및 시계...|\n",
      "|  의생활|   신변잡화|        시계전문점|1203|                각종 시계류 판매점|\n",
      "|  의생활|   신변잡화|            안경점|1204|        안경,콘택트렌즈,선글라스등|\n",
      "|  의생활|   신변잡화|            가방점|1205|         가방,핸드백,트렁크,지갑등|\n",
      "|  의생활|   신변잡화|            제화점|1206|           구두류 맞춤 및 판매업소|\n",
      "|  의생활|   신변잡화|        일반신발점|1207|  구두류를 제외한 각종 신발 및 ...|\n",
      "|  의생활|   신변잡화|        가발전문점|1208|  가발판매 전문점(미용재료업 제외)|\n",
      "+--------+-----------+------------------+----+----------------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "code_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
